{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccfcb37",
   "metadata": {},
   "source": [
    "# Indonesia Stock Exchange\n",
    "\n",
    "This will scrape two Indonesia Stock Exchange websites to categorise their announcements by financial event type and convert the contents of their PDFs to a table format.\n",
    "\n",
    "* idx.co.id\n",
    "* ksei.co.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e3b00",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from time import sleep\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pyautogui\n",
    "import ctypes\n",
    "import PyPDF2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691532ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None # filter SettingWithCopyWarning\n",
    "from pandas.tseries.offsets import BDay\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # filter UserWarningSettingWithCopyWarning\n",
    "import requests\n",
    "from base64 import b64encode\n",
    "import json\n",
    "import os\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8b388",
   "metadata": {},
   "source": [
    "## Function 1 - run_table\n",
    "\n",
    "Get announcements from idx.co.id and filter & group them based on the keywords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0d6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_table(mydate, checks = 'all'):\n",
    "    if mydate == \"today\":\n",
    "        mydate = dt.today().strftime(\"%Y%m%d\")\n",
    "    elif mydate == 'yesterday':\n",
    "        mydate = (dt.today() - BDay(1)).strftime(\"%Y%m%d\")\n",
    "    else:\n",
    "        mydate = dt.strptime(mydate, \"%d%m%Y\").strftime(\"%Y%m%d\")\n",
    "\n",
    "    # to count the total to check\n",
    "    total = 0\n",
    "    # collect history\n",
    "    history = []\n",
    "        \n",
    "    if checks == 'all':\n",
    "        with pd.ExcelFile(\"./backend/indonesia_keys.xlsx\") as keyfile:\n",
    "            checks = pd.read_excel(keyfile, 'Checks', header = None)[0].tolist()\n",
    "            keywords = \"|\".join(pd.read_excel(keyfile, 'Keywords', header = None)[0].tolist())\n",
    "            ignore = \"|\".join(pd.read_excel(keyfile, 'Ignore', header = None)[0].tolist())\n",
    "    else:\n",
    "        # since we wont be using the default keylist and inserting from the user's given list\n",
    "        with pd.ExcelFile(\"./backend/indonesia_keys.xlsx\") as keyfile:\n",
    "            keywords = \"|\".join(pd.read_excel(keyfile, 'Keywords', header = None)[0].tolist())\n",
    "            ignore = \"|\".join(pd.read_excel(keyfile, 'Ignore', header = None)[0].tolist())\n",
    "\n",
    "    # - Exporting as multiple sheets within a single excel\n",
    "    workbook = pd.ExcelWriter(f'output/Indonesia_Auto_{mydate}.xlsx', engine='xlsxwriter')\n",
    "\n",
    "    for ori_check in checks:\n",
    "        check = ori_check.strip().replace(\" \", \"+\")\n",
    "        # set link \n",
    "        the_link = f\"https://www.idx.co.id/primary/NewsAnnouncement/GetAllAnnouncement?keywords={check}&pageNumber=1&pageSize=2000&dateFrom={mydate}&dateTo={mydate}&lang=id\"\n",
    "\n",
    "        # request page with selenium\n",
    "        chrome_options = Options()\n",
    "        chrome_options.headless = True\n",
    "\n",
    "        # in case website error\n",
    "        active = True\n",
    "\n",
    "        while active:\n",
    "\n",
    "            dr = webdriver.Chrome(options=chrome_options)\n",
    "            dr.get(the_link)\n",
    "\n",
    "            sleep(5)\n",
    "\n",
    "            # Read HTML File\n",
    "            soup = BeautifulSoup(dr.page_source, \"html.parser\")\n",
    "\n",
    "            #dr.quit()\n",
    "\n",
    "            if (\"Verify you are human\" not in soup.find_all('body')[0].text) and (\"blocked\" not in soup.find_all('body')[0].text) and (\"Backend fetch failed\" not in soup.find_all('body')[0].text) and (len(soup.find_all('body')) != 0):\n",
    "                active = False\n",
    "\n",
    "        # get table\n",
    "        the_table = json.loads(soup.find_all('body')[0].text)['Items']\n",
    "        the_table = pd.DataFrame(the_table)\n",
    "\n",
    "        if len(the_table) != 0:\n",
    "            # change name of columns\n",
    "            col_name = {'Code':'symbol', 'PublishDate':'datetime', 'Title':'headline', 'Attachments':'det_desc'}\n",
    "            the_table = the_table.rename(columns = col_name)\n",
    "\n",
    "            # select relevant columns\n",
    "            the_table = the_table[['datetime', 'symbol', 'headline', 'det_desc']]\n",
    "\n",
    "            # change format of urls\n",
    "            for each in range(0, len(the_table)):\n",
    "                temp_links = \"\"\n",
    "                for i, link in enumerate(the_table.loc[each, 'det_desc']):\n",
    "                    if i == 0 and (link['FullSavePath'] not in history):\n",
    "                        the_table.loc[each, 'main_link'] = link['FullSavePath']\n",
    "                        history.append(link['FullSavePath'])\n",
    "                    elif i == 0 and (link['FullSavePath'] in history):\n",
    "                        the_table.loc[each, 'main_link'] = \"remove\"\n",
    "                    else:\n",
    "                        temp_links += link['FullSavePath']\n",
    "                        temp_links += \"\\n\"\n",
    "\n",
    "                    the_table.loc[each, 'other_links'] = temp_links\n",
    "\n",
    "            # select relevant columns\n",
    "            the_table = the_table[['datetime', 'symbol', 'headline', 'main_link', 'other_links']]\n",
    "                    \n",
    "            # trim whitespaces from symbol column\n",
    "            the_table['symbol'] = the_table['symbol'].str.strip()\n",
    "            \n",
    "            # remove those with no main link\n",
    "            the_table = the_table.loc[the_table['main_link'] != \"remove\"].reset_index(drop = True)\n",
    "\n",
    "            # get only those that contains keywords\n",
    "            the_table = the_table.loc[the_table['headline'].str.count(keywords, flags=re.IGNORECASE) != 0].reset_index(drop = True)\n",
    "\n",
    "            # get rid those with ignore\n",
    "            the_table = the_table.loc[the_table['headline'].str.count(ignore, flags=re.IGNORECASE) == 0].reset_index(drop = True)\n",
    "\n",
    "            if len(the_table) != 0:\n",
    "\n",
    "                the_table.to_excel(workbook, sheet_name=f'{ori_check[:30]}', index=False)\n",
    "                total += len(the_table)\n",
    "                print(f\"~ {ori_check}: {len(the_table)}\")\n",
    "\n",
    "    dr.quit()\n",
    "\n",
    "    workbook.close()\n",
    "    \n",
    "    print(f\"Total checks: {total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c031406",
   "metadata": {},
   "source": [
    "## Function 2 - get_ksei\n",
    "\n",
    "For KSEI, we just need to get the meetings announcement and create a template to ingest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ksei(mydate):\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    date = mydate.strftime('%m')\n",
    "    year = mydate.strftime('%Y')\n",
    "\n",
    "    the_link = f\"https://www.ksei.co.id/publications/corporate-action-schedules/meeting-announcement?Month={date}&Year={year}\"\n",
    "\n",
    "    # request page with selenium\n",
    "    chrome_options = Options()\n",
    "    chrome_options.headless = True\n",
    "\n",
    "    dr = webdriver.Chrome(options=chrome_options)\n",
    "    dr.maximize_window()\n",
    "    dr.get(the_link)\n",
    "\n",
    "    soup = BeautifulSoup(dr.page_source, \"html.parser\")\n",
    "\n",
    "    dr.close()\n",
    "\n",
    "    for tr in soup.find_all('table')[0].find_all('tr')[1:]:\n",
    "        link = \"https://www.ksei.co.id/\" + tr.find_all('td')[0].a['href']\n",
    "        date = tr.find_all('td')[2].text\n",
    "        the_list = [[link, date]]\n",
    "        data = pd.concat([data, pd.DataFrame(the_list, columns=['link', 'date'])], ignore_index = True)\n",
    "\n",
    "    # filter data\n",
    "    # Translate\n",
    "    # lower all text\n",
    "    data['date'] = data['date'].str.lower()\n",
    "    # change date language\n",
    "    # replace month from indonesian to english\n",
    "    ind_eng_months = {\"januari\":\"January\",\n",
    "                     \"februari\":\"February\",\n",
    "                     \"maret\":\"March\",\n",
    "                     #\"April\":\"April\",\n",
    "                     \"mei\":\"May\",\n",
    "                     \"juni\":\"June\",\n",
    "                     \"JUNI\":\"June\",\n",
    "                     \"juli\":\"July\",\n",
    "                     \"agustus\":\"August\",\n",
    "                     #\"September\":\"September\",\n",
    "                     \"oktober\":\"October\",\n",
    "                     #\"November\":\"November\",\n",
    "                     \"desember\":\"December\"}\n",
    "\n",
    "    for key, value in ind_eng_months.items():\n",
    "        # Replace key character with value character in string\n",
    "        try:\n",
    "            if len(data) != 0:\n",
    "                data['date'] = data['date'].str.replace(key, value)\n",
    "\n",
    "        except AttributeError:\n",
    "            break\n",
    "\n",
    "    # Convert Dates\n",
    "    if len(data) != 0:\n",
    "        data['date'] = pd.to_datetime(data['date'], format = 'mixed', dayfirst = True)\n",
    "        #filter\n",
    "        data = data.loc[data['date'].dt.date == mydate.date()].reset_index(drop = True)\n",
    "\n",
    "    links = data['link'].tolist()\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5505a",
   "metadata": {},
   "source": [
    "## Function 3 - get_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(issued_capital, idx_rups, ksei_rups, download_addr):\n",
    "\n",
    "    if len(issued_capital) != 0:\n",
    "\n",
    "        for link in issued_capital:\n",
    "            # request page with selenium\n",
    "            chrome_options = Options()\n",
    "            chrome_options.headless = True\n",
    "\n",
    "            #driver = webdriver.Chrome(options=chrome_options)\n",
    "            # in case website error\n",
    "            active = True\n",
    "\n",
    "            while active:\n",
    "                driver = webdriver.Edge()\n",
    "                driver.get(link)\n",
    "\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Read HTML File\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                #print(soup)\n",
    "                \n",
    "                if (\"Verify you are human\" not in soup.find_all('body')[0].text) and (\"blocked\" not in soup.find_all('body')[0].text) and (\"Backend fetch failed\" not in soup.find_all('body')[0].text) and (len(soup.find_all('body')) != 0):\n",
    "                    active = False\n",
    "\n",
    "                    pyautogui.hotkey('ctrl', 's')\n",
    "                    time.sleep(2)\n",
    "                    path = os.path.abspath(os.getcwd())+\"\\\\data\\\\Issued Capital\"\n",
    "                    path_and_filename = path+\"\\\\\"+link[-25:]\n",
    "                    #print(path_and_filename)\n",
    "                    pyautogui.typewrite(path_and_filename)\n",
    "                    pyautogui.press('enter')\n",
    "                    while any(filename.endswith(\"tmp\") for filename in os.listdir(download_addr)) or any(filename.endswith(\"tmp\") for filename in os.listdir(path)):\n",
    "                        sleep(1)\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "    if len(idx_rups) != 0:\n",
    "\n",
    "        for link in idx_rups:\n",
    "            # request page with selenium\n",
    "            chrome_options = Options()\n",
    "            chrome_options.headless = True\n",
    "\n",
    "            #driver = webdriver.Chrome(options=chrome_options)\n",
    "            # in case website error\n",
    "            active = True\n",
    "\n",
    "            while active:\n",
    "                driver = webdriver.Edge()\n",
    "                driver.get(link)\n",
    "\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Read HTML File\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                \n",
    "                if (\"Verify you are human\" not in soup.find_all('body')[0].text) and (\"blocked\" not in soup.find_all('body')[0].text) and (\"Backend fetch failed\" not in soup.find_all('body')[0].text) and (len(soup.find_all('body')) != 0):\n",
    "                    active = False\n",
    "\n",
    "                    pyautogui.hotkey('ctrl', 's')\n",
    "                    time.sleep(2)\n",
    "                    path = os.path.abspath(os.getcwd())+\"\\\\data\\\\IDX Meetings\"\n",
    "                    path_and_filename = path+\"\\\\\"+link[-25:]\n",
    "                    #print(path_and_filename)\n",
    "                    pyautogui.typewrite(path_and_filename)\n",
    "                    pyautogui.press('enter')\n",
    "                    while any(filename.endswith(\"tmp\") for filename in os.listdir(download_addr)) or any(filename.endswith(\"tmp\") for filename in os.listdir(path)):\n",
    "                        sleep(1)\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "    if len(ksei_rups) != 0:\n",
    "\n",
    "        for link in ksei_rups:\n",
    "            # request page with selenium\n",
    "            chrome_options = Options()\n",
    "            chrome_options.headless = True\n",
    "\n",
    "            #driver = webdriver.Chrome(options=chrome_options)\n",
    "            # in case website error\n",
    "            active = True\n",
    "\n",
    "            while active:\n",
    "                driver = webdriver.Edge()\n",
    "                driver.get(link)\n",
    "\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Read HTML File\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                \n",
    "                if (\"Verify you are human\" not in soup.find_all('body')[0].text) and (\"blocked\" not in soup.find_all('body')[0].text) and (\"Backend fetch failed\" not in soup.find_all('body')[0].text) and (len(soup.find_all('body')) != 0):\n",
    "                    active = False\n",
    "\n",
    "                    pyautogui.hotkey('ctrl', 's')\n",
    "                    time.sleep(2)\n",
    "                    path = os.path.abspath(os.getcwd())+\"\\\\data\\\\KSEI Meetings\"\n",
    "                    path_and_filename = path+\"\\\\\"+link[-37:]\n",
    "                    pyautogui.typewrite(path_and_filename)\n",
    "                    pyautogui.press('enter')\n",
    "                    while any(filename.endswith(\"tmp\") for filename in os.listdir(download_addr)) or any(filename.endswith(\"tmp\") for filename in os.listdir(path)):\n",
    "                        sleep(1)\n",
    "\n",
    "        driver.close()\n",
    "        \n",
    "    print(\"Acquired Documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fccb93",
   "metadata": {},
   "source": [
    "## Function 4 - conn_sql_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c099abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ID and issued capital details\n",
    "def conn_sql_isc(sql_username, sql_password, hostname = 'examplehost', portno = '0000', servicename = 'exampleservice'):\n",
    "    \n",
    "    # surpress warning\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    \n",
    "    # connect to SQL\n",
    "    dsn_tns = oracle.makedsn(hostname, portno, service_name=servicename) \n",
    "    conn = oracle.connect(user=sql_username, password=sql_password, dsn=dsn_tns) \n",
    "    \n",
    "    print(\"Username & Password Accepted\")\n",
    "    print(\"Generating Table\")\n",
    "    \n",
    "    c = conn.cursor()\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM random_table\n",
    "    \"\"\" \n",
    "\n",
    "    isscap_list = pd.read_sql_query(query, con=conn)\n",
    "    conn.close()\n",
    "\n",
    "    return isscap_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d88d9c",
   "metadata": {},
   "source": [
    "## Function 5 - gettable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93991f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettable():\n",
    "    # folder path\n",
    "    dir_path = os.path.join(os.getcwd(), \"data/Issued Capital\")\n",
    "\n",
    "    # list to store files\n",
    "    res = []\n",
    "\n",
    "    # Iterate directory\n",
    "    for path in os.listdir(dir_path):\n",
    "        # check if current path is a file\n",
    "        if os.path.isfile(os.path.join(dir_path, path)):\n",
    "            res.append(path)\n",
    "\n",
    "    # create empty dataframe\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    if len(res) != 0:\n",
    "        try:\n",
    "            for filename in res:\n",
    "                the_file = open(f\"./data/Issued Capital/{filename}\", 'rb')\n",
    "                the_pdf = PyPDF2.PdfReader(the_file)\n",
    "\n",
    "                #print(filename)\n",
    "\n",
    "                for i in range(0, len(the_pdf.pages)):\n",
    "                    #print(i)\n",
    "                    the_page = the_pdf.pages[i].extract_text().split('\\n')\n",
    "\n",
    "                    if i == 0 and ('Pencatatan Saham' == (the_page[1]).strip()):\n",
    "                        # Increase and Decrease\n",
    "\n",
    "                        try:\n",
    "                            the_dict = {'company': the_page[0],\n",
    "                                        'code': re.findall(r\"\\((\\S+)\\)\", the_page[0])[0], \n",
    "                                        'isc_change' : re.findall(r\"([\\d\\.]+)\", the_page[6])[0].replace(\".\", \"\"),\n",
    "                                        'isc_total' : re.findall(r\"([\\d\\.]+)\", the_page[7])[0].replace(\".\", \"\"),\n",
    "                                        'isc_date': the_page[8].replace(\"Tanggal Pencatatan dan Perdagangan\",\"\").strip(), } # Increase\n",
    "                            data = pd.concat([data, pd.DataFrame([the_dict])], ignore_index = True)\n",
    "\n",
    "                            the_dict = {'company': the_page[0],\n",
    "                                        'code': re.findall(r\"^([\\S\\-]+) [\\d\\,]+ [\\d\\,]+\", the_page[11])[0], \n",
    "                                        'isc_change' : \"-\"+re.findall(r\"^[\\S\\-]+ ([\\d\\,]+) [\\d\\,]+\", the_page[11])[0].replace(\",\", \"\"),\n",
    "                                        'isc_total' : re.findall(r\"^[\\S\\-]+ [\\d\\,]+ ([\\d\\,]+)\", the_page[11])[0].replace(\",\", \"\"),\n",
    "                                        'isc_date': the_page[8].replace(\"Tanggal Pencatatan dan Perdagangan\",\"\").strip(), } # Decrease\n",
    "                            data = pd.concat([data, pd.DataFrame([the_dict])], ignore_index = True)\n",
    "                        except:\n",
    "                            # limitation - other files that are not simple increase decrease from conversion updated manually\n",
    "                            t = re.findall(r\"\\((\\S+)\\)\", the_page[0])[0]\n",
    "                            print(f\"~ Manual Entry {t} - {filename}\")\n",
    "\n",
    "                    elif i == 0 and ('Pencatatan Saham' == (the_page[2]).strip()):\n",
    "                        # PUB\n",
    "                        break\n",
    "                    elif i == 0 and ('Pencatatan Saham' != (the_page[1]).strip()):\n",
    "                        break\n",
    "\n",
    "                the_file.close()\n",
    "            \n",
    "        except:\n",
    "            print(f\"Error in {filename}\")\n",
    "        \n",
    "        # lower all string\n",
    "        data['isc_date'] = data['isc_date'].str.lower()\n",
    "\n",
    "        # change date language\n",
    "        # replace month from indonesian to english\n",
    "        ind_eng_months = {\"januari\":\"January\",\n",
    "                         \"februari\":\"February\",\n",
    "                         \"maret\":\"March\",\n",
    "                         #\"April\":\"April\",\n",
    "                         \"mei\":\"May\",\n",
    "                         \"juni\":\"June\",\n",
    "                         \"JUNI\":\"June\",\n",
    "                         \"juli\":\"July\",\n",
    "                         \"agustus\":\"August\",\n",
    "                         #\"September\":\"September\",\n",
    "                         \"oktober\":\"October\",\n",
    "                         #\"November\":\"November\",\n",
    "                         \"desember\":\"December\"}\n",
    "\n",
    "        for key, value in ind_eng_months.items():\n",
    "            # Replace key character with value character in string\n",
    "            try:\n",
    "                data['isc_date'] = data['isc_date'].str.replace(key, value)\n",
    "            except AttributeError:\n",
    "                break\n",
    "\n",
    "        # set datetime\n",
    "        # get datetime object for indo\n",
    "        indodate = dt.now(timezone(\"Asia/Jakarta\"))\n",
    "        # get today's date for issued capital\n",
    "        data['date'] = indodate.strftime(\"%d/%m/%Y\")\n",
    "        # set indo time -20 mins\n",
    "        data['time'] = (indodate - timedelta(minutes = 20)).strftime(\"%H%M\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c1342",
   "metadata": {},
   "source": [
    "## Define Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eee695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indonesia_se(mydate, checks, sql_username, sql_password, docs_only = False):\n",
    "\n",
    "    print(\"_\"*50)\n",
    "    isc_sql = conn_sql_isc(sql_username, sql_password)\n",
    "\n",
    "    # get download address\n",
    "    the_path = os.getcwd()\n",
    "    while \"\\\\Parent Folder\" in the_path:\n",
    "        the_path = os.path.split(the_path)[0]\n",
    "\n",
    "    download_addr = the_path + \"\\\\Downloads\"\n",
    "    \n",
    "    if docs_only == False:\n",
    "        run_table(mydate, checks)\n",
    "        \n",
    "    if mydate == \"today\":\n",
    "        mydate = dt.today()\n",
    "    elif mydate == 'yesterday':\n",
    "        mydate = (dt.today() - BDay(1))\n",
    "    else:\n",
    "        mydate = dt.strptime(mydate, \"%d%m%Y\")\n",
    "            \n",
    "    if docs_only == False:\n",
    "        # remove docs in file\n",
    "        folder_names = ['IDX Meetings', 'Issued Capital', 'KSEI Meetings']\n",
    "        for folder in folder_names:\n",
    "            for file in os.listdir(f\"./data/{folder}/\"):\n",
    "                os.remove(f\"./data/{folder}/\" + file)\n",
    "        \n",
    "        doc_date = mydate.strftime(\"%Y%m%d\")\n",
    "        with pd.ExcelFile(f'output/Indonesia_Auto_{doc_date}.xlsx') as excel_file:\n",
    "            if 'Pencatatan Saham  ' in excel_file.sheet_names:\n",
    "                issued_capital = pd.read_excel(excel_file, 'Pencatatan Saham  ', header = 0)['main_link'].tolist()\n",
    "            else:\n",
    "                issued_capital = []\n",
    "            if 'Rapat Umum' in excel_file.sheet_names:\n",
    "                idx_rups = pd.read_excel(excel_file, 'Rapat Umum', header = 0)['main_link'].tolist()\n",
    "            else:\n",
    "                idx_rups = []\n",
    "\n",
    "        ksei_rups = get_ksei(mydate)\n",
    "\n",
    "        print(f\"KSEI Meetings: {len(ksei_rups)}\")\n",
    "        ctypes.windll.user32.MessageBoxW(None, \"Please step away from the computer so we can get documents\", \"Indonesia Auto Validation\", 1)\n",
    "\n",
    "        print(\"_\"*50)\n",
    "        get_docs(issued_capital, idx_rups, ksei_rups, download_addr)\n",
    "        \n",
    "    print(\"Creating Bulks\")\n",
    "    thedate = mydate.strftime(\"%Y%m%d\")\n",
    "    # check if today's compare file exists\n",
    "    whole_path = Path(f\"./output/Indonesia_Auto_{thedate}.xlsx\")\n",
    "\n",
    "    if whole_path.exists():\n",
    "        with pd.ExcelWriter(f\"./output/Indonesia_Auto_{thedate}.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"overlay\") as writer:\n",
    "            guide = pd.DataFrame(['Manual Updates are all changes and cancellation', 'Cancellation does not have Record Date', 'There is no cancellation cases for IDX', 'Those that have no ID or ID is code means its not found'])\n",
    "            guide.to_excel(writer, sheet_name='Guidance', index=False, header = None)\n",
    "    else:\n",
    "        workbook = pd.ExcelWriter(f'./output/Indonesia_Auto_{thedate}.xlsx', engine='xlsxwriter')\n",
    "        guide = pd.DataFrame(['Manual Updates are all changes and cancellation', 'Cancellation does not have Record Date', 'There is no cancellation cases for IDX', 'Those that have no ID or ID is code means its not found'])\n",
    "        guide.to_excel(workbook, sheet_name='Guidance', index=False, header = None)\n",
    "        workbook.close()\n",
    "\n",
    "    print(\"Process complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
